**Stochastic [[Gradient]] Descent**
Méthode d'optimisation pour ajuster numériquement des valeurs.

Elle utilise un sous ensemble aléatoire pour chaque mise à jour ce qui rend l'[[Learning Technique|apprentissage]] plus rapide que la descente de [[Gradient|gradient]] classique

![[Pasted image 20251218211141.png]]

**Exemple d'utilisation :** ajuster les poids d'un [[Neural networks|réseau de neurones]] 
# $$W\leftarrow W-\eta\nabla_WE$$
